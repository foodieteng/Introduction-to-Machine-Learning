{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IagZMs0_qjdL"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "Welcome to your third assignment. In this assignment, you will build a deep neural network step by step. In this notebook, you will implement all the functions required to build a neural network.\n",
        "\n",
        "After finishing this assignment, you will have a deeper understanding of the process of training a deep neural network, which only consists of three steps: forward propagation, backward propagation and update."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGFR00CQvoaH"
      },
      "source": [
        "# 2. Packages\n",
        "All the packages that you need to finish this assignment are listed below.\n",
        "*   numpy : the fundamental package for scientific computing with Python.\n",
        "*   matplotlib : a comprehensive library for creating static, animated, and interactive visualizations in Python.\n",
        "*   math : Python has a built-in module that you can use for mathematical tasks.\n",
        "*   sklearn.datasets : scikit-learn comes with a few small standard datasets that do not require to download any file from some external website. You will be using the Iris dataset to build a binary classifier.\n",
        "*   pandas.read_csv : provides functionality for reading a csv dataset from a GitHub repository.\n",
        "\n",
        "‚ö†Ô∏è **WARNING** ‚ö†Ô∏è: \n",
        "*   Please do not import any other packages.\n",
        "*   np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don't change the seed.\n",
        "\n",
        "üí° **Reminder** üí°: The basic part only includes binary classification. Functions like `softmax()` and `compute_CCE_loss` are counted as the bonus part, you can skip these implementations if you wish to do the basic part only üòÄ. If you are not sure which part belongs to the bonus part, you can refer to the code. For the bonus part, the code will look as follows: `### START CODE HERE ### (...) (bonus)`. Please set `bonus` to True in the first code cell if you want to do the bonus part.\n",
        "\n",
        "‚ùó **Important** ‚ùó: Please do not change the code outside this code bracket.\n",
        "```\n",
        "### START CODE HERE ### (‚âà n lines of code)\n",
        "...\n",
        "### END CODE HERE ###\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fmTH9UkeqdYf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn import datasets\n",
        "from pandas import read_csv\n",
        "\n",
        "output = {}\n",
        "\n",
        "\"\"\"\n",
        "Set bonus to True if you want to do the bonus part\n",
        "\"\"\"\n",
        "### START CODE HERE ### (‚âà 1 line of code) (bonus)\n",
        "bonus = True\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vsx_EzPf58rf"
      },
      "outputs": [],
      "source": [
        "# Helper function\n",
        "def predict(X, y, parameters, classes):\n",
        "    \"\"\"\n",
        "    This function is used to predict the results of a  L-layer neural network.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data set of examples you would like to label\n",
        "    parameters -- parameters of the trained model\n",
        "    classes - number of classes, 2 for binary classification, >2 for multi-class classification\n",
        "    \n",
        "    Returns:\n",
        "    p -- predictions for the given dataset X\n",
        "    \"\"\"\n",
        "    #.shape[0]->row ; .shape[1]->column\n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    if classes == 2:\n",
        "      #‰∏ÄÂàómË°åÁöÑÔºê\n",
        "      p = np.zeros((1,m))\n",
        "    else:\n",
        "      p = np.zeros((classes, m))\n",
        "    \n",
        "    # Forward propagation\n",
        "    probas, caches = L_model_forward(X, parameters, classes)\n",
        "    \n",
        "    if classes == 2:\n",
        "      # convert probas to 0/1 predictions\n",
        "      for i in range(0, probas.shape[1]):\n",
        "          if probas[0,i] > 0.5:\n",
        "              p[0,i] = 1\n",
        "          else:\n",
        "              p[0,i] = 0\n",
        "\n",
        "      #print results\n",
        "      if y is not None:\n",
        "        print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
        "\n",
        "    else:\n",
        "      # convert probas to one hot vector predictions\n",
        "      prediction = np.argmax(probas, axis=0, out=None)\n",
        "    \n",
        "      for i in range(len(prediction)):\n",
        "          p[prediction[i], i] = 1\n",
        "\n",
        "      #print results\n",
        "      if y is not None:\n",
        "        correct = 0\n",
        "        for i in range(m):\n",
        "          if (p[:, i] == y[:, i]).all():\n",
        "            correct += 1\n",
        "        print(\"Accuracy: \"  + str(correct/m))\n",
        "        \n",
        "    return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t0BnaDRzYil"
      },
      "source": [
        "# 3. Forward propagation module\n",
        "## 3.1. Implement a linear layer\n",
        "In this section, you will need to implement a linear layer. A linear layer applies a linear transformation to the incoming data:\n",
        "$Z = WA + b$, where $W$ and $b$ are the weight and bias.\n",
        "\n",
        "**Note**: Fully-connected layers, also known as linear layers, connect every input neuron to every output neuron and are commonly used in neural networks.\n",
        "\n",
        "### 3.1.1. Initialize parameters (1-layer neural network)\n",
        "**Exercise**: Create and initialize parameters of a linear layer with He initialization. (5%)\n",
        "\n",
        "\"He Initialization\" is named for the first author of He et al., 2015. (If you have heard of \"Xavier initialization\", this is similar except Xavier initialization uses a scaling factor for the weights $W^{[l]}$ of sqrt(1./layers_dims[l-1]) where He initialization would use sqrt(2./layers_dims[l-1]).)\n",
        "\n",
        "**Instructions**:\n",
        "*   Use random initialization (normal distribution) for the weight matrices and multiply it by $\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}$.\n",
        "*   Use zero initialization for the biases and multiply it by $\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x0KHo8w9yqbY"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: initialize_parameters\n",
        "\n",
        "def initialize_parameters(n_x, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_y -- size of the output layer\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_y, n_x)\n",
        "                    b1 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "\n",
        "    ### START CODE HERE ### (‚âà 2 lines of code)\n",
        "    ### np.random.randn(row, column)\n",
        "    W1 = np.random.randn(n_y, n_x) * np.sqrt(2/n_x)\n",
        "    b1 = np.zeros((n_y, 1)) * np.sqrt(2/n_x)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    ###Âà§Êñ∑ÊòØÂê¶ÁÇ∫expressionÊòØÂê¶ÊàêÁ´ã\n",
        "    assert(W1.shape == (n_y, n_x))\n",
        "    assert(b1.shape == (n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1}\n",
        "    \n",
        "    \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HNAWwmg8R7T",
        "outputId": "d2d5429b-7824-4030-c207-1bba26bbdc0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W1 = [[ 1.32627244 -0.49949702 -0.43125043]]\n",
            "b1 = [[0.]]\n"
          ]
        }
      ],
      "source": [
        "parameters = initialize_parameters(3,1)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "\n",
        "output[\"initialize_parameters\"] = initialize_parameters(4, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtPtH0j3BFN7"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>W1: </td>\n",
        "    <td>[[ 1.32627244 -0.49949702 -0.43125043]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b1: </td>\n",
        "    <td>[[0.]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztHAQoBC-seU"
      },
      "source": [
        "### 3.1.2. Initialize parameters (L-layer neural network)\n",
        "**Exercise**: Create and initialize parameters for an L-layer neural network with He initialization. (5%)\n",
        "\n",
        "**Instructions**:\n",
        "*   Use random initialization (normal distribution) for the weight matrices and multiply it by $\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}$.\n",
        "*   Use zero initialization for the biases and multiply it by $\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DEAIJKJ7-iQz"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: initialize_parameters_deep\n",
        "\n",
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        ### START CODE HERE ### (‚âà 2 lines of code)\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/layer_dims[l-1])\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1)) * np.sqrt(2/layer_dims[l-1])\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPSp12IZ_j38",
        "outputId": "17e511dd-0974-4c13-9b87-48809baf1567"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W1 = [[ 1.02732621 -0.38690873 -0.33404515 -0.67860494  0.54733184]\n",
            " [-1.45562088  1.10351585 -0.48142952  0.20177804 -0.15771567]\n",
            " [ 0.92471825 -1.30294739 -0.20391454 -0.2428973   0.71705876]\n",
            " [-0.69563232 -0.10905317 -0.55520641  0.02669832  0.36860471]]\n",
            "b1 = [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "W2 = [[-0.77825528  0.8094419   0.63752091  0.35531715]\n",
            " [ 0.63700135 -0.48346861 -0.08689651 -0.66168891]\n",
            " [-0.18942548  0.37501795 -0.48907801 -0.28054711]]\n",
            "b2 = [[0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ],
      "source": [
        "parameters = initialize_parameters_deep([5,4,3])\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
        "\n",
        "output[\"initialize_parameters_deep\"] = initialize_parameters_deep([3,4,5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC38k01CC0Nn"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>W1: </td>\n",
        "    <td>[[ 1.02732621 -0.38690873 -0.33404515 -0.67860494  0.54733184]\n",
        " [-1.45562088  1.10351585 -0.48142952  0.20177804 -0.15771567]\n",
        " [ 0.92471825 -1.30294739 -0.20391454 -0.2428973   0.71705876]\n",
        " [-0.69563232 -0.10905317 -0.55520641  0.02669832  0.36860471]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b1: </td>\n",
        "    <td>[[0.]\n",
        " [0.]\n",
        " [0.]\n",
        " [0.]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>W2: </td>\n",
        "    <td>[[-0.77825528  0.8094419   0.63752091  0.35531715]\n",
        " [ 0.63700135 -0.48346861 -0.08689651 -0.66168891]\n",
        " [-0.18942548  0.37501795 -0.48907801 -0.28054711]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b2: </td>\n",
        "    <td>[[0.]\n",
        " [0.]\n",
        " [0.]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abu7YqxeAeMz"
      },
      "source": [
        "### 3.1.3. Linear forward\n",
        "\n",
        "After initializing parameters, you will need to apply the linear transformation to the incoming data, and this can be simply done by matrix multiplication and addition.\n",
        "\n",
        "**Exercise**: Apply the linear transformation. (5%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O_oGueTE8X61"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: linear_forward\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter \n",
        "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "\n",
        "    ### Z = WA + b\n",
        "    ### START CODE HERE ### (‚âà 1 line of code)\n",
        "    Z = np.dot(W, A) + b\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSf8JIyjaj_A",
        "outputId": "2577301e-2856-4921-f516-b1308725267e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Z = [[1.9 2.2 2.5]]\n"
          ]
        }
      ],
      "source": [
        "A, W, b = np.array([[0, 0.5, 1], [1, 1.5, 2], [2, 2.5, 3]]), np.array([[0.1, 0.2, 0.3]]), np.array([[1.1]])\n",
        "\n",
        "Z, linear_cache = linear_forward(A, W, b)\n",
        "print(\"Z = \" + str(Z))\n",
        "\n",
        "output[\"linear_forward\"] = linear_forward(np.array([[0, -0.5, -1], [1, 1.5, 2], [-2, -2.5, -3]]), np.array([[0.5, 0.3, 0.7]]), np.array([[-1.1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpcPlE8-EUsR"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Z: </td>\n",
        "    <td>[[1.9 2.2 2.5]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syt1bV3bdI_f"
      },
      "source": [
        "## 3.2. Activation function layer\n",
        "\n",
        "In this section, you will need to implement activation function layers. There are many activation functions, such as sigmoid function, softmax function, ReLU function and etc. \n",
        "\n",
        "### 3.2.1. Sigmoid function\n",
        "Sigmoid: $\\sigma(Z) = \\begin{cases}\n",
        "    \\frac{1}{1+e^{-Z}},& \\text{if } Z >= 0\\\\\n",
        "    \\frac{e^{Z}}{1+e^{Z}}, & \\text{otherwise}\n",
        "\\end{cases}$. \n",
        "\n",
        "‚ùó**Important**‚ùó: As you can see, there is an exponential function inside the sigmoid function, so you might encounter an exponential overflow problem when implementing this function. To solve this problem, we use the numerically stable sigmoid function as shown in the equation above.\n",
        "\n",
        "### 3.2.2. Softmax function\n",
        "Softmax: $\\sigma(\\vec{Z})_i = \\frac{e^{Z_i-b}}{\\sum_{j=1}^{K} e^{Z_j-b}}$, where $\\vec{Z}$ = input vector, $K$ = number of classes in the multi-class classifier, $b$ is $\\max_{j=1}^{K} Z_j$\n",
        "\n",
        "‚ùó**Important**‚ùó: The naive implementation $\\sigma(\\vec{Z})_i = \\frac{e^{Z_i}}{\\sum_{j=1}^{K} e^{Z_j}}$ is terrible when there are large numbers! You might encounter the following problems if you use the naive implementation.\n",
        "*   RuntimeWarning: overflow encountered in exp\n",
        "\n",
        "\n",
        "### 3.2.3. ReLU (rectified linear unit) function\n",
        "ReLU: $RELU(Z) = max(Z, 0)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PkLKaFWiWmF"
      },
      "source": [
        "**Exercise**: Implement activation function. (10%+5%) (basic: Sigmoid and ReLU, bonus: Softmax)\n",
        "\n",
        "**Instruction**: \n",
        "*   Sigmoid: This function returns two items: the activation value \"a\" and a cache \"a\" contains \"z\" (it's what we will feed in to the corresponding backward function).\n",
        "*   Softmax: This function returns two items: the activation value \"a\" and a cache \"a\" contains \"z\" (it's what we will feed in to the corresponding backward function).\n",
        "*   ReLU: This function returns two items: the activation value \"a\" and a cache \"a\" contains \"z\" (it's what we will feed in to the corresponding backward function)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Nnuv8MmebMgg"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: sigmoid, softmax, relu\n",
        "\n",
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Implements the sigmoid activation in numpy\n",
        "    \n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape\n",
        "    \n",
        "    Returns:\n",
        "    A -- output of sigmoid(z), same shape as Z\n",
        "    cache -- returns Z as well, useful during backpropagation\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (‚âà 8 lines of code)\n",
        "    A = 1/(1 + np.exp(-Z))\n",
        "    cache = Z\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return A, cache\n",
        "\n",
        "def softmax(Z):\n",
        "    \"\"\"\n",
        "    Implements the softmax activation in numpy\n",
        "    \n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape (dim 0: number of classes, dim 1: number of samples)\n",
        "    \n",
        "    Returns:\n",
        "    A -- output of softmax(z), same shape as Z\n",
        "    cache -- returns Z as well, useful during backpropagation\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (‚âà 2 lines of code) (bonus)\n",
        "    exp = np.exp(Z)\n",
        "    A = exp/(exp.sum(0))\n",
        "    cache = Z\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Implement the RELU function in numpy\n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape\n",
        "    Returns:\n",
        "    A -- output of relu(z), same shape as Z\n",
        "    cache -- returns Z as well, useful during backpropagation\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (‚âà 2 lines of code)\n",
        "    A = np.maximum(0,Z)\n",
        "    cache = Z\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    return A, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBuRAoeUC5jV",
        "outputId": "5cf4aacb-42eb-4da3-aaab-57b8434266fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sigmoid: A = [[0.00669285 0.26894142 0.5        0.73105858 0.99330715]]\n",
            "ReLU: A = [[0 0 0 1 5]]\n",
            "Softmax: A = \n",
            "[[0.0320586  0.1748777  0.0320586 ]\n",
            " [0.08714432 0.47536689 0.08714432]\n",
            " [0.23688282 0.1748777  0.23688282]\n",
            " [0.64391426 0.1748777  0.64391426]]\n"
          ]
        }
      ],
      "source": [
        "Z = np.array([[-5, -1, 0, 1, 5]])\n",
        "\n",
        "A, cache = sigmoid(Z)\n",
        "print(\"Sigmoid: A = \" + str(A))\n",
        "output[\"sigmoid\"] = sigmoid(np.array([[-1.82, -0.71, 0.02, 0.13, 2.21]]))\n",
        "\n",
        "A, cache = relu(Z)\n",
        "print(\"ReLU: A = \" + str(A))\n",
        "output[\"relu\"] = relu(np.array([[-1.82, -0.71, 0.02, 0.13, 2.21]]))\n",
        "\n",
        "if bonus:\n",
        "  Z = np.array([[1, 0, -2], [2, 1, -1], [3, 0, 0], [4, 0, 1]])\n",
        "  A, cache = softmax(Z)\n",
        "  print(\"Softmax: A = \\n\" + str(A))\n",
        "  output[\"softmax\"] = softmax(np.array([[0.1, 1.2, -2.1], [2.2, 0.7, -1.3], [1.4, 0.3, 0.2], [3.9, 0.5, -1.6]]))\n",
        "else:\n",
        "  output[\"softmax\"] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyyX_xxdEmNp"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>(With sigmoid) A: </td>\n",
        "    <td>[[0.00669285 0.26894142 0.5        0.73105858 0.99330715]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With ReLU) A: </td>\n",
        "    <td>[[0 0 0 1 5]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With softmax) A: </td>\n",
        "    <td>[[0.0320586  0.1748777  0.0320586 ]\n",
        " [0.08714432 0.47536689 0.08714432]\n",
        " [0.23688282 0.1748777  0.23688282]\n",
        " [0.64391426 0.1748777  0.64391426]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcNYrvQHi5TZ"
      },
      "source": [
        "## 3.3. Model forward\n",
        "Alright, now you have all the tools that are needed to build a forward model. Let's get started! üòÄ\n",
        "\n",
        "### 3.3.1. Linear activation forward\n",
        "First, you will need to combine linear and activation function layers into a linear-activation layer, following this equation: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]}+b^{[l]})$, where activation function $g$ could be `sigmoid()`, `softmax()` or `relu()`.\n",
        "\n",
        "**Exercise**: Implement linear-activation layer. (5%)\n",
        "\n",
        "**Note**: In deep learning, a linear-activation layer is counted as a single layer in the neural network, not two layers since the activation layer does not have any parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0JGMzfIDCSVz"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: linear_activation_forward\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\", \"relu\" or \"softmax\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value \n",
        "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        ### START CODE HERE ### (‚âà 2 lines of code)\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b) # This \"linear_cache\" contains (A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z) # This \"activation_cache\" contains \"Z\"\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        ### START CODE HERE ### (‚âà 2 lines of code)\n",
        "        Z, linear_cache = linear_forward(A_prev ,W, b) # This \"linear_cache\" contains (A_prev, W, b)\n",
        "        A, activation_cache = relu(Z) # This \"activation_cache\" contains \"Z\"\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    elif activation == \"softmax\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        ### START CODE HERE ### (‚âà 2 lines of code)  (bonus)\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b) # This \"linear_cache\" contains (A_prev, W, b)\n",
        "        A, activation_cache = softmax(Z) # This \"activation_cache\" contains \"Z\"\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yVQQqe2EyHA",
        "outputId": "ca81c877-09b3-4997-bf92-f5170a8111f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With sigmoid: A = [[0.64565631 0.20915937 0.77902611]]\n",
            "With ReLU: A = [[0.6  0.   1.26]]\n",
            "With softmax: A = \n",
            "[[0.47535001 0.05272708 0.68692136]\n",
            " [0.14317267 0.75380161 0.05526942]\n",
            " [0.38147732 0.19347131 0.25780921]]\n"
          ]
        }
      ],
      "source": [
        "A_prev, W, b = np.array([[0.1, -1.2, 1.9], [1.1, 0.2, 2.3], [2.9, -2.5, 3.7]]), np.array([[0.1, 0.2, 0.3]]), np.array([[-0.5]])\n",
        "\n",
        "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
        "print(\"With sigmoid: A = \" + str(A))\n",
        "output[\"linear_activation_forward_sigmoid\"] = linear_activation_forward(np.array([[1.1, -2.2], [-3.9, 0.6]]), np.array([[9.1, -8.2]]), np.array([[0.5]]), activation = \"sigmoid\")\n",
        "\n",
        "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
        "print(\"With ReLU: A = \" + str(A))\n",
        "output[\"linear_activation_forward_relu\"] = linear_activation_forward(np.array([[1.1, -2.2], [-3.9, 0.6]]), np.array([[9.1, -8.2]]), np.array([[0.5]]), activation = \"relu\")\n",
        "\n",
        "if bonus:\n",
        "  A_prev, W, b = np.array([[0.1, -1.2, 1.9], [1.1, 0.2, 2.3], [2.9, -2.5, 3.7]]), np.array([[0.1, 0.2, 0.3], [-0.1, -0.2, -0.3], [-0.1, 0, 0.1]]), np.array([[-0.5], [0.5], [0.1]])\n",
        "  A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"softmax\")\n",
        "  print(\"With softmax: A = \\n\" + str(A))\n",
        "  output[\"linear_activation_forward_softmax\"] = linear_activation_forward(np.array([[-0.1, 1.2, 1.9], [-1.1, 0.2, -2.3], [2.9, -2.5, -3.7]]), np.array([[0.2, 0.2, 0.2], [-0.1, -0.1, -0.1], [-0.1, 0, 0.1]]), np.array([[-0.1], [0.1], [0.5]]), activation = \"softmax\")\n",
        "else:\n",
        "  output[\"linear_activation_forward_softmax\"] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMkf2ss6F52W"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>(With sigmoid) A: </td>\n",
        "    <td>[[0.64565631 0.20915937 0.77902611]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With ReLU) A: </td>\n",
        "    <td>[[0.6  0.   1.26]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With softmax) A: </td>\n",
        "    <td>[[0.47535001 0.05272708 0.68692136]\n",
        " [0.14317267 0.75380161 0.05526942]\n",
        " [0.38147732 0.19347131 0.25780921]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJVlZeyNAu-y"
      },
      "source": [
        "### 3.3.2. L model forward\n",
        "For even more convenience when implementing the $L$-layer Neural Net, you will need a function that replicates the previous one (linear_activation_forward with `relu()`) $L-1$ times, then follows that with one linear_activation_forward with `sigmoid()` for binary classification or `softmax()` for multi-class classification.\n",
        "\n",
        "In the code below, the variable AL will denote $A^{[L]} = g(Z^{[L]}) = g(W^{[L]}A^{[L-1]}+b^{[L]})$, where activation function g could be `sigmoid()` for binary classification or `softmax()` for multi-class classification.\n",
        "\n",
        "**Instruction**:\n",
        "*   Use the functions you had previously written.\n",
        "*   Use a for loop to replicate [LINEAR->RELU] (L-1) times.\n",
        "*   Don't forget to keep track of the caches in the \"caches\" list. To add a new value c to a list, you can use list.append(c).\n",
        "\n",
        "**Note**: There are N nodes in the last layer for N-class classification, but only one node for binary classification. Intuitively, this could be pretty confusing sometimes since there should be two nodes in the last layer for binary classification. However, both the one-node(sigmoid, binary cross-entropy) and two-node(softmax, categorical cross-entropy) techniques for binary classification work fine, and picking one technique over the other is a matter of subjective preference. For this assignment, you will implement the former one, which is what we usually do for binary classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "k9jRgHX-FTpQ"
      },
      "outputs": [],
      "source": [
        "def L_model_forward(X, parameters, classes):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "    classes -- number of classes (2 for binary classification, >2 for multi-class classification) \n",
        "    \n",
        "    Returns:\n",
        "    AL -- last post-activation value\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    if classes == 2:\n",
        "      # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "      AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
        "      caches.append(cache)\n",
        "      assert(AL.shape == (1,X.shape[1]))\n",
        "    else:\n",
        "      # Implement LINEAR -> SOFTMAX. Add \"cache\" to the \"caches\" list.\n",
        "      AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"softmax\")\n",
        "      caches.append(cache)\n",
        "      assert(AL.shape == (classes,X.shape[1]))\n",
        "            \n",
        "    return AL, caches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s26LVkCbIbJ3",
        "outputId": "4147bea9-79d6-4702-a723-1ca171a86ea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AL = [[0.07802314 0.01818575 0.00403778]]\n",
            "Length of caches list = 2\n",
            "AL = [[6.18255541e-03 7.61028230e-04 7.66374121e-05]\n",
            " [2.11940510e-01 2.01651685e-01 1.56962913e-01]\n",
            " [1.35096366e-02 2.03338206e-03 2.50381145e-04]\n",
            " [2.18485590e-02 6.14321609e-03 1.41311042e-03]\n",
            " [2.66899863e-01 4.51947825e-01 6.26088267e-01]\n",
            " [1.13256434e-01 1.07946425e-01 8.41707040e-02]\n",
            " [7.27765014e-02 3.08118586e-02 1.06721503e-02]\n",
            " [2.17220658e-02 4.88048793e-03 8.97083101e-04]\n",
            " [4.19482610e-02 1.32235761e-02 3.41028905e-03]\n",
            " [2.29915615e-01 1.80600516e-01 1.16058465e-01]]\n",
            "Length of caches list = 2\n"
          ]
        }
      ],
      "source": [
        "# binary classification\n",
        "X, parameters, classes = np.array([[0, 1, 2], [-2, -1, 0], [0.5, 0.5, 0.5]]), initialize_parameters_deep([3,3,1]), 2\n",
        "AL, caches = L_model_forward(X, parameters, classes)\n",
        "print(\"AL = \" + str(AL))\n",
        "print(\"Length of caches list = \" + str(len(caches)))\n",
        "\n",
        "# multi-class classification\n",
        "if bonus:\n",
        "  X, parameters, classes = np.array([[0, 1, 2], [-2, -1, 0], [0.5, 0.5, 0.5]]), initialize_parameters_deep([3,3,10]), 10\n",
        "  AL, caches = L_model_forward(X, parameters, classes)\n",
        "  print(\"AL = \" + str(AL))\n",
        "  print(\"Length of caches list = \" + str(len(caches)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE8RRd2iEInv",
        "outputId": "09be23f8-9896-483d-d08b-e1af92495104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 1  0 -2]\n",
            " [ 2  1 -1]\n",
            " [ 3  0  0]\n",
            " [ 4  0  1]]\n"
          ]
        }
      ],
      "source": [
        "print(cache)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoCdrONOHhvw"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>(Binary classification) AL: </td>\n",
        "    <td>[[0.07802314 0.01818575 0.00403778]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(Binary classification) Length of caches list: </td>\n",
        "    <td>2</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(Multi-class classification) AL: </td>\n",
        "    <td>[[6.18255541e-03 7.61028230e-04 7.66374121e-05]\n",
        " [2.11940510e-01 2.01651685e-01 1.56962913e-01]\n",
        " [1.35096366e-02 2.03338206e-03 2.50381145e-04]\n",
        " [2.18485590e-02 6.14321609e-03 1.41311042e-03]\n",
        " [2.66899863e-01 4.51947825e-01 6.26088267e-01]\n",
        " [1.13256434e-01 1.07946425e-01 8.41707040e-02]\n",
        " [7.27765014e-02 3.08118586e-02 1.06721503e-02]\n",
        " [2.17220658e-02 4.88048793e-03 8.97083101e-04]\n",
        " [4.19482610e-02 1.32235761e-02 3.41028905e-03]\n",
        " [2.29915615e-01 1.80600516e-01 1.16058465e-01]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(Multi-class classification) Length of caches list: </td>\n",
        "    <td>2</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmSBVaQOSRrk"
      },
      "source": [
        "# 4. Cost function\n",
        "In this section, you will implement the cost function. We use binary cross-entropy loss for binary classification and categorical cross-entropy loss for multi-class classification. You need to compute the cost, because you want to check if your model is actually learning. Cross-entropy loss is minimized, where smaller values represent a better model than larger values. A model that predicts perfect probabilities has a cross entropy or log loss of 0.0.\n",
        "\n",
        "## 4.1. Binary cross-entropy loss\n",
        "**Exercise**: Compute the binary cross-entropy cost $J$, using the following formula: (5%) $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MjBT0eYQaY81"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: compute_BCE_cost\n",
        "\n",
        "def compute_BCE_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the binary cross-entropy cost function using the above formula.\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- binary cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    ### START CODE HERE ### (‚âà 1 line of code)\n",
        "    cost = 1./m * np.nansum(np.multiply(-np.log(AL),Y) + np.multiply(-np.log(1 - AL), 1 - Y))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HACHles_EIn8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r07sqnIXaaMv",
        "outputId": "b8f1a1d6-f219-421f-ad2f-0d96d8510821"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cost = 0.5784031417840181\n"
          ]
        }
      ],
      "source": [
        "AL, Y = np.array([[0.9, 0.6, 0.4, 0.1, 0.2, 0.8]]), np.array([[1, 1, 1, 0, 0, 0]])\n",
        "\n",
        "print(\"cost = \" + str(compute_BCE_cost(AL, Y)))\n",
        "output[\"compute_BCE_cost\"] = compute_BCE_cost(np.array([[0.791, 0.983, 0.654, 0.102, 0.212, 0.091, 0.476, 0.899]]), np.array([[1, 1, 1, 1, 0, 0, 0, 0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iRtgOx_IGPo"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>cost: </td>\n",
        "    <td>0.5784031417840181</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aealRyKbcQzG"
      },
      "source": [
        "## 4.2. Categorical cross-entropy loss\n",
        "**Exercise**: Compute the categorical cross-entropy cost $J$, using the following formula: (5%) $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right))$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Owx-kTdcfxV5"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: compute_CCE_cost\n",
        "\n",
        "def compute_CCE_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the categorical cross-entropy cost function using the above formula.\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (number of classes, number of examples)\n",
        "    Y -- true \"label\" vector (one hot vector, for example: [[1], [0], [0]] represents rock, [[0], [1], [0]] represents paper, [[0], [0], [1]] represents scissors \n",
        "                              in a Rock-Paper-Scissors image classification), shape (number of classes, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- categorical cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    ### START CODE HERE ### (‚âà 1 line of code) (bonus)\n",
        "    cost = -(np.log(AL)*Y).sum()/m\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YbHVAc7hSh3",
        "outputId": "fbe9abd1-9ce7-4f56-ee2f-6f35868b8833"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cost = 0.472268904012926\n"
          ]
        }
      ],
      "source": [
        "if bonus:\n",
        "  AL, Y = np.array([[0.8, 0.6, 0.4, 0.1, 0.2, 0.4], [0.1, 0.3, 0.5, 0.7, 0.1, 0.1], [0.1, 0.1, 0.1, 0.2, 0.7, 0.5]]), np.array([[1, 1, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 1, 1]])\n",
        "  print(\"cost = \" + str(compute_CCE_cost(AL, Y)))\n",
        "  output[\"compute_CCE_cost\"] = compute_CCE_cost(np.array([[0.711, 0.001, 0.11], [0.099, 0.217, 0.09], [0.035, 0.599, 0.12], [0.068, 0.123, 0.1], [0.087, 0.06, 0.58]]), np.array([[1, 0, 0], [0, 0, 0], [0, 1, 0], [0, 0, 0], [0, 0, 1]]))\n",
        "else:\n",
        "  output[\"compute_CCE_cost\"] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9VVIBB5Ic-D"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>cost: </td>\n",
        "    <td>0.472268904012926</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuplx8dBeZtO"
      },
      "source": [
        "# 5. Backward propagation module\n",
        "In this section, you will implement helper functions for backpropagation. Remember that back propagation is used to calculate the gradient of the loss function with respect to the parameters.\n",
        "\n",
        "A quick revision for backpropapagation:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/amanchadha/coursera-deep-learning-specialization/6ebc4d60a90c1592aee3eaa3113eb4b37d9b4b19/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step/images/backprop_kiank.png\" height=\"200\"/>\n",
        "\n",
        "Similar to forward propagation, you are going to build the backward propagation in three steps:\n",
        "*   LINEAR backward\n",
        "*   LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU, sigmoid or softmax activation\n",
        "*   [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID/SOFTMAX backward (whole model)\n",
        "\n",
        "## 5.1. Linear backward\n",
        "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
        "\n",
        "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n",
        "\n",
        "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)} $$$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n",
        "\n",
        "**Exercise**: Use the 3 formulas above to implement `linear_backward()`. (5%)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "k5HL2LG6eeVn"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: linear_backward\n",
        "\n",
        "def linear_backward(dZ, cache):\n",
        "    # Here cache is \"linear_cache\" containing (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    ### START CODE HERE ### (‚âà 3 lines of code)\n",
        "    dW = 1./m * np.dot(dZ, A_prev.T)\n",
        "    db = 1./m * np.sum(dZ, axis=1, keepdims = True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJfNFIbF4RvH",
        "outputId": "553f91bc-16ff-4efd-dd1e-4778235397ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dA_prev = [[3.5 6. ]]\n",
            "dW = [[1.625]\n",
            " [0.625]]\n",
            "db = [[2.  ]\n",
            " [0.75]]\n"
          ]
        }
      ],
      "source": [
        "# Set up some test inputs\n",
        "dZ, linear_cache = np.array([[1.5, 2.5], [0.5, 1.0]]), (np.array([[0.5, 1]]), np.array([[2.0], [1.0]]), np.array([[0.5], [1.0]]))\n",
        "\n",
        "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "print (\"dA_prev = \" + str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db))\n",
        "\n",
        "output[\"linear_backward\"] = linear_backward(np.array([[0.5, -1.5], [-1.5, 2.0]]), (np.array([[0.25, 1.25]]), np.array([[-1.0], [1.0]]), np.array([[-0.5], [-1.0]])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U91CObMIrP_"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>dA_prev: </td>\n",
        "    <td>[[3.5 6. ]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW: </td>\n",
        "    <td>[[1.625]\n",
        " [0.625]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db: </td>\n",
        "    <td>[[2.  ]\n",
        " [0.75]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3CBdcyZJHdF"
      },
      "source": [
        "## 5.2. Linear-Activation backward\n",
        "Next, you will create a function that merges the two helper functions: `linear_backward()` and the backward step for the activation `linear_activation_backward()`.\n",
        "\n",
        "First, you will need to implement the backward functions of `sigmoid()`, `relu()` and `softmax()`+`compute_CCE_cost`.\n",
        "\n",
        "**Exercise**: Implement backward function. (10%+5%) (basic: Sigmoid and ReLU, bonus: Softmax+CCE_loss)\n",
        "\n",
        "**Instruction**:\n",
        "*   sigmoid_backward: Implements the backward propagation for SIGMOID unit.\n",
        "*   relu_backward: Implements the backward propagation for RELU unit.\n",
        "*   softmax_CCE_backward: Implements the backward propagation for [SOFTMAX->LOSS] unit.\n",
        "\n",
        "If $g(.)$ is the activation function, sigmoid_backward, relu_backward and softmax_backward compute$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
        "\n",
        "1. The derivative of the sigmoid function is: $$œÉ^{'}(Z^{[l]}) = œÉ(Z^{[l]}) (1 - œÉ(Z^{[l]}))$$. <br>\n",
        "‚ùó**Important**‚ùó: You should use the numerically stable sigmoid function to prevent the overflow exponential problem. \n",
        "\n",
        "2. The derivative of the relu function is: $$g'(Z^{[l]}) = \\begin{cases}\n",
        "    1,& \\text{if } Z^{[l]}> 0\\\\\n",
        "    0,              & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        "3. TLDRüòâ: The derivative of the categorical cross-entropy loss with respect to the last hidden layer is: $$\\frac{\\partial \\mathcal{L}}{\\partial Z} = s - y $$. <br> The derivative of the softmax function is: $$\\frac{\\partial S(z_i)}{\\partial z_j} = \\begin{cases}\n",
        "    S(z_i) \\times (1 - S(z_i)),& \\text{if } i = j\\\\\n",
        "    -S(z_i) \\times S(z_j),              & \\text{if } i \\neq j\n",
        "\\end{cases}$$, where $z$ is a vector with shape (number of classes K, 1) and $S(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$. Hence, the real derivative of softmax function would be a full Jacobian matrix. For the special case, K = 4, we have <img src=\"https://miro.medium.com/max/554/1*SWfgFQLDIPXDf1C6CHmr8A.png\" height=\"100\"/>. <br> It is quite complicated to calculate the softmax derivative on its own. However, if you use the softmax and the cross entropy loss, that complexity fades away. Since the softmax layer is usually used at the output, we can actually calculate the derivative of the categorical cross-entropy loss with respect to the n-th node in the last hidden layer. Instead of a long clunky formula, you end up with this terse, easy to compute thing: $$\\frac{\\partial \\mathcal{L}}{\\partial Z_i} = s_i - y_i $$, where $s$ is the output of the softmax function and the $y$ is the true label vector(one-hot vector). For more information, you can refer to this article [Derivative of the Softmax Function and the Categorical Cross-Entropy Loss](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1). <br> \n",
        "‚ùó**Important**‚ùó: The above mathematical derivation is based on naive implementation. In order to deal with the exponential overflow problem, we should use the normalized exponential function when counting $s$. For the sake of simplicity, we just use the same gradient equation as the naive implementation.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "m9gGiDslLqZr"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: sigmoid_backward, relu_backward, softmax_CCE_backward\n",
        "\n",
        "def sigmoid_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single SIGMOID unit.\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (‚âà 9 lines of code)\n",
        "    Z = cache\n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (‚âà 3 lines of code)\n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object. \n",
        "    dZ[Z <= 0] = 0 # When z <= 0, you should set dz to 0 as well.\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def softmax_CCE_backward(Y, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a [SOFTMAX->LOSS] unit.\n",
        "    Arguments:\n",
        "    Y -- true \"label\" vector (one hot vector, for example: [[1], [0], [0]] represents rock, [[0], [1], [0]] represents paper, [[0], [0], [1]] represents scissors \n",
        "                              in a Rock-Paper-Scissors image classification), shape (number of classes, number of examples)\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (‚âà 3 lines of code) (bonus)\n",
        "    A, Z = softmax(cache)\n",
        "    dZ = A - Y\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0OG6KtNyYZY",
        "outputId": "3c17bfb4-3d10-4e7e-c179-b3fcd868d55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sigmoid: dZ = [[-0.5        -0.26935835 -0.11969269 -0.5        -0.73139639]]\n",
            "ReLU: dZ = [[ 0.    0.   -1.14]\n",
            " [ 1.7   0.    3.72]]\n",
            "Softmax: dZ = [[-0.96488097  0.09003057  0.01766842]\n",
            " [ 0.70538451 -0.75527153  0.01766842]\n",
            " [ 0.25949646  0.66524096 -0.03533684]]\n"
          ]
        }
      ],
      "source": [
        "dA, cache = np.array([[-2, -1.37, -1.14, -2, -3.72]]), np.array([[0, 1, 2, 0, 1]])\n",
        "dZ = sigmoid_backward(dA, cache)\n",
        "print(\"Sigmoid: dZ = \"+ str(dZ))\n",
        "output[\"sigmoid_backward\"] = sigmoid_backward(np.array([[-2, -2, -1.37, -1.14, -3.72]]), np.array([[2, 0, 1.5, 0, 0.5]]))\n",
        "\n",
        "dA, cache = np.array([[-2, -1.37, -1.14], [1.7, 2, 3.72]]), np.array([[-2, -1, 2], [1, 0, 1]])\n",
        "dZ = relu_backward(dA, cache)\n",
        "print(\"ReLU: dZ = \"+ str(dZ))\n",
        "output[\"relu_backward\"] = relu_backward(np.array([[3.179, -1.376, -0.114], [2.227, -5.612, 4.172]]), np.array([[0.53, 1.21, -2.22], [-1.58, 0.99, -0.11]]))\n",
        "\n",
        "if bonus:\n",
        "  Y, cache = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([[-2, -1, -2], [1, 0, -2], [0, 1, 2]])\n",
        "  dZ = softmax_CCE_backward(Y, cache)\n",
        "  print(\"Softmax: dZ = \" + str(dZ))\n",
        "  output[\"softmax_CCE_backward\"] = softmax_CCE_backward(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([[-2.11, -1.22, -2.33], [1.44, 0.55, -2.66], [0.77, 1.88, 2.99]]))\n",
        "else:\n",
        "  output[\"softmax_CCE_backward\"] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQtJYDZs3ODH"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>(With sigmoid) dZ: </td>\n",
        "    <td>[[-0.5        -0.26935835 -0.11969269 -0.5        -0.73139639]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With ReLU) dZ: </td>\n",
        "    <td>[[ 0.    0.   -1.14]\n",
        " [ 1.7   0.    3.72]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With softmax) dZ: </td>\n",
        "    <td>[[-0.96488097  0.09003057  0.01766842]\n",
        " [ 0.70538451 -0.75527153  0.01766842]\n",
        " [ 0.25949646  0.66524096 -0.03533684]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SLLGIfLLUkq"
      },
      "source": [
        "**Exercise**: Implement the backpropagation for the LINEAR->ACTIVATION layer. (5%)\n",
        "\n",
        "**Instruction**:\n",
        "*   Use the functions you had previously written."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CTwbEEG14Emy"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: linear_activation_backward\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l \n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        ### START CODE HERE ### (‚âà 1 line of code)\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        ### START CODE HERE ### (‚âà 1 line of code)\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    ### START CODE HERE ### (‚âà 1 line of code)\n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odZeIDj942uR",
        "outputId": "633f458b-f4f8-4f7a-c6f1-b31d0850c57d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sigmoid:\n",
            "dA_prev = [[-1.         -0.5387167  -0.23938537 -1.         -1.46279278]\n",
            " [-0.5        -0.26935835 -0.11969269 -0.5        -0.73139639]]\n",
            "dW = [[-0.13868689  0.13868689]]\n",
            "db = [[-0.42408949]]\n",
            "\n",
            "relu:\n",
            "dA_prev = [[3.4  0.   9.72]\n",
            " [1.7  0.   4.86]\n",
            " [0.   0.   0.  ]]\n",
            "dW = [[ 0.          0.         -0.76      ]\n",
            " [-1.13333333  1.13333333  2.48      ]]\n",
            "db = [[-0.38      ]\n",
            " [ 1.80666667]]\n"
          ]
        }
      ],
      "source": [
        "dAL, linear_activation_cache  = np.array([[-2, -1.37, -1.14, -2, -3.72]]), ((np.array([[-2, -1, 0, 1, 2], [2, 1, 0, -1, -2]]), np.array([[2.0, 1.0]]), np.array([[0.5]])), np.array([[0, 1, 2, 0, 1]]))\n",
        "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache , activation = \"sigmoid\")\n",
        "print (\"sigmoid:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db) + \"\\n\")\n",
        "output[\"linear_activation_backward_sigmoid\"] = linear_activation_backward(dAL+1, linear_activation_cache , activation = \"sigmoid\")\n",
        "\n",
        "dAL, linear_activation_cache  = np.array([[-2, -1.37, -1.14], [1.7, 2, 3.72]]), ((np.array([[-2, -1, 0], [2, 1, 0], [0, 1, 2]]), np.array([[-2, -1, 0], [2, 1, 0]]), np.array([[0.5], [-0.5]])), np.array([[-2, -1, 2], [1, 0, 1]]))\n",
        "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache , activation = \"relu\")\n",
        "print (\"relu:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db))\n",
        "output[\"linear_activation_backward_relu\"] = linear_activation_backward(dAL+0.5, linear_activation_cache , activation = \"relu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1YFVdD17nGL"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Sigmoid </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA_prev: </td>\n",
        "    <td>[[-1.         -0.5387167  -0.23938537 -1.         -1.46279278]\n",
        " [-0.5        -0.26935835 -0.11969269 -0.5        -0.73139639]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW: </td>\n",
        "    <td>[[-0.13868689  0.13868689]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db: </td>\n",
        "    <td>[[-0.42408949]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>ReLU </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA_prev: </td>\n",
        "    <td>[[3.4  0.   9.72]\n",
        " [1.7  0.   4.86]\n",
        " [0.   0.   0.  ]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW: </td>\n",
        "    <td>[[ 0.          0.         -0.76      ]\n",
        " [-1.13333333  1.13333333  2.48      ]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db: </td>\n",
        "    <td>[[-0.38      ]\n",
        " [ 1.80666667]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOFlw70x88A6"
      },
      "source": [
        "##5.3 - L-Model Backward\n",
        "Now you will implement the backward function for the whole network. Recall that when you implemented the `L_model_forward` function, at each iteration, you stored a cache which contains (X,W,b, and z). In the back propagation module, you will use those variables to compute the gradients. Therefore, in the `L_model_backward` function, you will iterate through all the hidden layers backward, starting from layer $L$. On each step, you will use the cached values for layer $l$ to backpropagate through layer $l$. Figure below shows the backward pass.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/amanchadha/coursera-deep-learning-specialization/6ebc4d60a90c1592aee3eaa3113eb4b37d9b4b19/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step/images/mn_backward.png\" height=\"400\"/>\n",
        "\n",
        "Initializing backpropagation: To backpropagate through this network, we know that the output is, $A^{[L]} = \\sigma(Z^{[L]})$. Your code thus needs to compute dAL $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$. To do so, use this formula (derived using calculus which you don't need in-depth knowledge of):\n",
        "```\n",
        "dAL = - (np.divide(Y, AL + œµ) - np.divide(1 - Y, 1 - AL + œµ)) # derivative of cost with respect to AL, where œµ = 1e-5 is added to prevent zero division.\n",
        "```\n",
        "\n",
        "\n",
        "You can then use this post-activation gradient dAL to keep going backward. As seen in figure above, you can now feed in dAL into the LINEAR->SIGMOID backward function you implemented (which will use the cached values stored by the L_model_forward function). After that, you will have to use a for loop to iterate through all the other layers using the LINEAR->RELU backward function. You should store each dA, dW, and db in the grads dictionary. To do so, use this formula :\n",
        "\n",
        "$$grads[\"dW\" + str(l)] = dW^{[l]} $$\n",
        "For example, for $l=3$ this would store $dW^{[l]}$ in grads[\"dW3\"].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "IPnv1BZd_GfF"
      },
      "outputs": [],
      "source": [
        "def L_model_backward(AL, Y, caches, classes):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    classes -- number of classes (2 for binary classification, >2 for multi-class classification)\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    if classes == 2:\n",
        "      # Initializing the backpropagation\n",
        "      dAL = - (np.divide(Y, AL + 1e-5) - np.divide(1 - Y, 1 - AL + 1e-5))\n",
        "      \n",
        "      # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "      current_cache = caches[L-1] # Last Layer\n",
        "      grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
        "    else:\n",
        "      # Initializing the backpropagation\n",
        "      linear_cache, activation_cache = caches[L-1] # Last Layer\n",
        "      dZ = softmax_CCE_backward(Y, activation_cache)\n",
        "      \n",
        "      # Lth layer (LINEAR) gradients. Inputs: \"dZ, linear_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "      grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jaRO9SvCdEY",
        "outputId": "938290b0-4ee9-42b1-cb5e-b4e27b65d2cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary classification\n",
            "dW1 = [[-0.00178234 -0.12389888  0.03052914]\n",
            " [ 0.          0.          0.        ]\n",
            " [-0.01472455 -1.0235744   0.25221246]]\n",
            "db1 = [[0.06105827]\n",
            " [0.        ]\n",
            " [0.50442493]]\n",
            "dA1 = [[ 1.87699706e-01 -3.70276293e-03 -8.22125021e-04]\n",
            " [-1.10052058e+00  2.17100329e-02  4.82028195e-03]\n",
            " [ 1.55065655e+00 -3.05898912e-02 -6.79187823e-03]]\n",
            "\n",
            "Multi-class classification\n",
            "dW1 = [[ 0.42051843  0.06541362  0.0887762 ]\n",
            " [ 0.          0.          0.        ]\n",
            " [ 1.08372031 -1.61959581  0.67582903]]\n",
            "db1 = [[0.1775524 ]\n",
            " [0.        ]\n",
            " [1.35165806]]\n",
            "dA1 = [[-0.09507462 -0.00609164  0.63382346]\n",
            " [-1.45707343  0.00732963 -0.17178251]\n",
            " [ 2.4424153  -0.02604318  1.63860206]]\n"
          ]
        }
      ],
      "source": [
        "# binary classification\n",
        "X, parameters, classes = np.array([[0, 1, 2], [-2, -1, 0], [0.5, 0.5, 0.5]]), initialize_parameters_deep([3,3,1]), 2\n",
        "AL, caches = L_model_forward(X, parameters, classes)\n",
        "Y_assess = np.array([[1, 0, 0]])\n",
        "grads = L_model_backward(AL, Y_assess, caches, classes)\n",
        "print(\"Binary classification\")\n",
        "print(\"dW1 = \"+ str(grads[\"dW1\"]))\n",
        "print(\"db1 = \"+ str(grads[\"db1\"]))\n",
        "print(\"dA1 = \"+ str(grads[\"dA1\"]) +\"\\n\")\n",
        "\n",
        "# multi-class classification\n",
        "if bonus:\n",
        "  X, parameters, classes = np.array([[0, 1, 2], [-2, -1, 0], [0.5, 0.5, 0.5]]), initialize_parameters_deep([3,3,3]), 3\n",
        "  AL, caches = L_model_forward(X, parameters, classes)\n",
        "  Y_assess = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
        "  grads = L_model_backward(AL, Y_assess, caches, classes)\n",
        "  print(\"Multi-class classification\")\n",
        "  print(\"dW1 = \"+ str(grads[\"dW1\"]))\n",
        "  print(\"db1 = \"+ str(grads[\"db1\"]))\n",
        "  print(\"dA1 = \"+ str(grads[\"dA1\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-pzrTaOE8tL"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Binary classification </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW1: </td>\n",
        "    <td>[[-0.00178234 -0.12389888  0.03052914]\n",
        " [ 0.          0.          0.        ]\n",
        " [-0.01472455 -1.0235744   0.25221246]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db1: </td>\n",
        "    <td>[[0.06105827]\n",
        " [0.        ]\n",
        " [0.50442493]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA1: </td>\n",
        "    <td>[[ 1.87699706e-01 -3.70276293e-03 -8.22125021e-04]\n",
        " [-1.10052058e+00  2.17100329e-02  4.82028195e-03]\n",
        " [ 1.55065655e+00 -3.05898912e-02 -6.79187823e-03]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Multi-class classification </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW1: </td>\n",
        "    <td>[[ 0.42051843  0.06541362  0.0887762 ]\n",
        " [ 0.          0.          0.        ]\n",
        " [ 1.08372031 -1.61959581  0.67582903]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db1: </td>\n",
        "    <td>[[0.1775524 ]\n",
        " [0.        ]\n",
        " [1.35165806]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA1: </td>\n",
        "    <td>[[-0.09507462 -0.00609164  0.63382346]\n",
        " [-1.45707343  0.00732963 -0.17178251]\n",
        " [ 2.4424153  -0.02604318  1.63860206]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MkxS_itFkV3"
      },
      "source": [
        "# 6. Update parameters\n",
        "In this section you will update the parameters of the model, using gradient descent:\n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n",
        "where $\\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary.\n",
        "\n",
        "**Exercise**: Implement update_parameters() to update your parameters using gradient descent. (5%)\n",
        "\n",
        "**Instructions**: \n",
        "*   Update parameters using gradient descent on every $W^{[l]}$ and $b^{[l]}$ for $l = 1, 2, ..., L$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GZZtqaXlFpYP"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: update_parameters\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    ### START CODE HERE ### (‚âà 3 lines of code)\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "    ### END CODE HERE ###\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfKajEa9HjDO",
        "outputId": "454f5111-7490-4f39-a647-8cc0b6fab8d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W1 = [[ 0.39721186  0.64025004 -0.09671178  0.27099015]\n",
            " [ 0.07752363  0.00469968  0.09679955  0.33705631]\n",
            " [ 0.392862    0.52183369  0.33138026  0.67538482]]\n",
            "b1 = [[ 0.16234149]\n",
            " [ 0.78232848]\n",
            " [-0.02592894]]\n",
            "W2 = [[0.6012798  0.38575324 0.49003974]]\n",
            "b2 = [[0.05692437]]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1)\n",
        "parameters, grads = {\"W1\": np.random.rand(3, 4), \"b1\": np.random.rand(3,1), \"W2\": np.random.rand(1,3), \"b2\": np.random.rand(1,1)}, {\"dW1\": np.random.rand(3, 4), \"db1\": np.random.rand(3,1), \"dW2\": np.random.rand(1,3), \"db2\": np.random.rand(1,1)}\n",
        "parameters = update_parameters(parameters, grads, 0.1)\n",
        "\n",
        "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
        "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
        "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
        "print (\"b2 = \"+ str(parameters[\"b2\"]))\n",
        "\n",
        "np.random.seed(1)\n",
        "output[\"update_parameters\"] = update_parameters({\"W1\": np.random.randn(3, 4), \"b1\": np.random.randn(3,1), \"W2\": np.random.randn(1,3), \"b2\": np.random.randn(1,1)}, {\"dW1\": np.random.randn(3, 4), \"db1\": np.random.randn(3,1), \"dW2\": np.random.randn(1,3), \"db2\": np.random.randn(1,1)}, 0.075)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_WoEl0NMyhO"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>W1: </td>\n",
        "    <td>[[ 0.39721186  0.64025004 -0.09671178  0.27099015]\n",
        " [ 0.07752363  0.00469968  0.09679955  0.33705631]\n",
        " [ 0.392862    0.52183369  0.33138026  0.67538482]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b1: </td>\n",
        "    <td>[[ 0.16234149]\n",
        " [ 0.78232848]\n",
        " [-0.02592894]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>W2: </td>\n",
        "    <td>[[0.6012798  0.38575324 0.49003974]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b2: </td>\n",
        "    <td>[[0.05692437]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpQah0JDdMyl"
      },
      "source": [
        "# Basic implementation (binary classification)\n",
        "\n",
        "Congratulations on implementing all the functions by yourself. You have done an incredible job! üëè\n",
        "\n",
        "Now you have all the tools you need to get started with classification. In this section, you will build a binary classifier using the functions you had previously written. You will create a model that can classify the different species of the Iris flower. The Iris dataset consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. However, you only need to classify Iris setosa and Iris versicolor in this exercise.\n",
        "\n",
        "**Exercise**: Implement a binary classifier and tune hyperparameter. (10%)\n",
        "\n",
        "**Instruction**:\n",
        "*   Train a model with validation accuracy higher than 80%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "fI7JY5ESjhZ2"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: L_layer_model\n",
        "\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False, classes=2):\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID. (binary)\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX. (multi-class)\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (4, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if setosa, 1 if versicolor), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    classes -- number of classes, 2 for binary classification, >2 for multi-class classification\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization. (‚âà 1 line of code)\n",
        "    ### START CODE HERE ###\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID/SOFTMAX.\n",
        "        ### START CODE HERE ### (‚âà 1 line of code)\n",
        "        AL, caches = L_model_forward(X, parameters, classes)\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Compute cost.\n",
        "        if classes == 2:\n",
        "          ### START CODE HERE ### (‚âà 1 line of code)\n",
        "          cost = compute_BCE_cost(AL, Y)\n",
        "          ### END CODE HERE ###\n",
        "        else:\n",
        "          ### START CODE HERE ### (‚âà 1 line of code) (bonus)\n",
        "          cost = compute_CCE_cost(AL, Y)\n",
        "          ### END CODE HERE ###\n",
        "    \n",
        "        # Backward propagation.\n",
        "        ### START CODE HERE ### (‚âà 1 line of code)\n",
        "        grads = L_model_backward(AL, Y, caches, classes)\n",
        "        ### END CODE HERE ###\n",
        " \n",
        "        # Update parameters.\n",
        "        ### START CODE HERE ### (‚âà 1 line of code)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "        ### END CODE HERE ###\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq6fq-A7dPLG",
        "outputId": "9e46fad3-a1fa-4099-9414-d1dcf466b604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of X: (4, 100)\n",
            "shape of y: (1, 100)\n",
            "shape of X_train: (4, 90) shape of y_train: (1, 90)\n",
            "shape of X_val: (4, 10) shape of y_val: (1, 10)\n"
          ]
        }
      ],
      "source": [
        "# load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:100].T\n",
        "y = np.expand_dims(iris.target[:100], axis=1).T\n",
        "\n",
        "print(\"shape of X: \" + str(X.shape))\n",
        "print(\"shape of y: \" + str(y.shape))\n",
        "\n",
        "# split training set and validation set\n",
        "X_train, y_train = np.concatenate((X[:, :45], X[:, 50:95]), axis=1), np.concatenate((y[:, :45], y[:, 50:95]), axis=1)\n",
        "X_val, y_val = np.concatenate((X[:, 45:50], X[:, 95:]), axis=1), np.concatenate((y[:, 45:50], y[:, 95:]), axis=1)\n",
        "\n",
        "print(\"shape of X_train: \" + str(X_train.shape) + \" shape of y_train: \" + str(y_train.shape))\n",
        "print(\"shape of X_val: \" + str(X_val.shape) + \" shape of y_val: \" + str(y_val.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZDYhhVTDwA8K",
        "outputId": "d82f06fe-1868-4f37-b03c-d800a693b082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 1.041683\n",
            "Cost after iteration 100: 0.510580\n",
            "Cost after iteration 200: 0.248112\n",
            "Cost after iteration 300: 0.142816\n",
            "Cost after iteration 400: 0.093088\n",
            "Cost after iteration 500: 0.066486\n",
            "Cost after iteration 600: 0.050570\n",
            "Cost after iteration 700: 0.040238\n",
            "Cost after iteration 800: 0.033104\n",
            "Cost after iteration 900: 0.027935\n",
            "Cost after iteration 1000: 0.024047\n",
            "Cost after iteration 1100: 0.021033\n",
            "Cost after iteration 1200: 0.018637\n",
            "Cost after iteration 1300: 0.016694\n",
            "Cost after iteration 1400: 0.015090\n",
            "Cost after iteration 1500: 0.013746\n",
            "Cost after iteration 1600: 0.012606\n",
            "Cost after iteration 1700: 0.011629\n",
            "Cost after iteration 1800: 0.010782\n",
            "Cost after iteration 1900: 0.010042\n",
            "Cost after iteration 2000: 0.009391\n",
            "Cost after iteration 2100: 0.008813\n",
            "Cost after iteration 2200: 0.008298\n",
            "Cost after iteration 2300: 0.007837\n",
            "Cost after iteration 2400: 0.007421\n",
            "Cost after iteration 2500: 0.007044\n",
            "Cost after iteration 2600: 0.006701\n",
            "Cost after iteration 2700: 0.006388\n",
            "Cost after iteration 2800: 0.006101\n",
            "Cost after iteration 2900: 0.005838\n",
            "Cost after iteration 3000: 0.005594\n",
            "Cost after iteration 3100: 0.005370\n",
            "Cost after iteration 3200: 0.005161\n",
            "Cost after iteration 3300: 0.004967\n",
            "Cost after iteration 3400: 0.004787\n",
            "Cost after iteration 3500: 0.004618\n",
            "Cost after iteration 3600: 0.004460\n",
            "Cost after iteration 3700: 0.004312\n",
            "Cost after iteration 3800: 0.004173\n",
            "Cost after iteration 3900: 0.004043\n",
            "Cost after iteration 4000: 0.003919\n",
            "Cost after iteration 4100: 0.003803\n",
            "Cost after iteration 4200: 0.003693\n",
            "Cost after iteration 4300: 0.003589\n",
            "Cost after iteration 4400: 0.003490\n",
            "Cost after iteration 4500: 0.003396\n",
            "Cost after iteration 4600: 0.003307\n",
            "Cost after iteration 4700: 0.003222\n",
            "Cost after iteration 4800: 0.003141\n",
            "Cost after iteration 4900: 0.003064\n",
            "Cost after iteration 5000: 0.002990\n",
            "Cost after iteration 5100: 0.002920\n",
            "Cost after iteration 5200: 0.002853\n",
            "Cost after iteration 5300: 0.002788\n",
            "Cost after iteration 5400: 0.002726\n",
            "Cost after iteration 5500: 0.002667\n",
            "Cost after iteration 5600: 0.002610\n",
            "Cost after iteration 5700: 0.002556\n",
            "Cost after iteration 5800: 0.002504\n",
            "Cost after iteration 5900: 0.002453\n",
            "Cost after iteration 6000: 0.002405\n",
            "Cost after iteration 6100: 0.002358\n",
            "Cost after iteration 6200: 0.002313\n",
            "Cost after iteration 6300: 0.002269\n",
            "Cost after iteration 6400: 0.002228\n",
            "Cost after iteration 6500: 0.002187\n",
            "Cost after iteration 6600: 0.002148\n",
            "Cost after iteration 6700: 0.002110\n",
            "Cost after iteration 6800: 0.002074\n",
            "Cost after iteration 6900: 0.002038\n",
            "Cost after iteration 7000: 0.002004\n",
            "Cost after iteration 7100: 0.001971\n",
            "Cost after iteration 7200: 0.001939\n",
            "Cost after iteration 7300: 0.001908\n",
            "Cost after iteration 7400: 0.001877\n",
            "Cost after iteration 7500: 0.001848\n",
            "Cost after iteration 7600: 0.001820\n",
            "Cost after iteration 7700: 0.001792\n",
            "Cost after iteration 7800: 0.001765\n",
            "Cost after iteration 7900: 0.001739\n",
            "Cost after iteration 8000: 0.001714\n",
            "Cost after iteration 8100: 0.001689\n",
            "Cost after iteration 8200: 0.001665\n",
            "Cost after iteration 8300: 0.001642\n",
            "Cost after iteration 8400: 0.001619\n",
            "Cost after iteration 8500: 0.001597\n",
            "Cost after iteration 8600: 0.001575\n",
            "Cost after iteration 8700: 0.001554\n",
            "Cost after iteration 8800: 0.001533\n",
            "Cost after iteration 8900: 0.001513\n",
            "Cost after iteration 9000: 0.001494\n",
            "Cost after iteration 9100: 0.001475\n",
            "Cost after iteration 9200: 0.001456\n",
            "Cost after iteration 9300: 0.001438\n",
            "Cost after iteration 9400: 0.001420\n",
            "Cost after iteration 9500: 0.001403\n",
            "Cost after iteration 9600: 0.001386\n",
            "Cost after iteration 9700: 0.001370\n",
            "Cost after iteration 9800: 0.001353\n",
            "Cost after iteration 9900: 0.001338\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# GRADED CODE: hyperparameter tuning\n",
        "# suggestion: 1-2 layers, layer node < 100, learning rate < 0.01\n",
        "### START CODE HERE ### (‚âà 2 lines of code)\n",
        "layers_dims = [X_train.shape[0], 2, 1] # e.g. [4, 1]\n",
        "parameters = L_layer_model(X_train, y_train, layers_dims, learning_rate = 0.0075, num_iterations = 10000, print_cost = True, classes=2)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkeoJrFZznMf",
        "outputId": "5376146e-19ce-47d8-a582-882af5d0b66f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "pred_train = predict(X_train, y_train, parameters, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mERo3g41zsyX",
        "outputId": "cb89cd59-bccf-4b26-9712-cb00a1b46736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "pred_val = predict(X_val, y_val, parameters, 2)\n",
        "output[\"basic_pred_val\"] = pred_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnVkyjtC62K8"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Accuracy: </td>\n",
        "    <td>>= 0.8</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMCpPFMVdj36"
      },
      "source": [
        "# Bonus implementation (multi class classification)\n",
        "\n",
        "In this section, you need to implement a multi-class classifier using the functions you had previously written. You will create a model that can classify four hand gestures using electromyography (EMG) signals. The EMG signal is a biomedical signal that measures electrical currents generated in muscles during its contraction representing neuromuscular activities.\n",
        "\n",
        "**Exercise**: Implement a multi-class classifier and tune hyperparameter. (10%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVSfqnXqXGdC",
        "outputId": "889a910b-733b-4fe8-be30-c72d771c487b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of X_train: (64, 9342)\n",
            "shape of y_train: (4, 9342)\n",
            "shape of X_test: (64, 2336)\n"
          ]
        }
      ],
      "source": [
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "# load data\n",
        "X_train = read_csv(\"https://raw.githubusercontent.com/ivanlim123/Assignment-3-Backpropagation/main/X_train.csv\", header=None).to_numpy()\n",
        "y_train = read_csv(\"https://raw.githubusercontent.com/ivanlim123/Assignment-3-Backpropagation/main/y_train.csv\", header=None).to_numpy()\n",
        "X_test = read_csv(\"https://raw.githubusercontent.com/ivanlim123/Assignment-3-Backpropagation/main/X_test.csv\", header=None).to_numpy()\n",
        "\n",
        "print(\"shape of X_train: \" + str(X_train.shape))\n",
        "print(\"shape of y_train: \" + str(y_train.shape))\n",
        "print(\"shape of X_test: \" + str(X_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ljAcf2tpQDR-"
      },
      "outputs": [],
      "source": [
        "#You can split training and validation set here. (Optional)\n",
        "if bonus:\n",
        "### START CODE HERE ###\n",
        "    split = int(X_train.shape[1] * 0.8)\n",
        "    X_training = X_train[:, :split]\n",
        "    y_training = y_train[:, :split]\n",
        "    X_validate = X_train[:, split:]\n",
        "    y_validate = y_train[:, split:]\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HYD-qRs7doU0",
        "outputId": "cfa6e1b0-ec38-4acf-b0c6-f99ee49c797c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 2.215174\n",
            "Cost after iteration 100: 1.448886\n",
            "Cost after iteration 200: 1.400192\n",
            "Cost after iteration 300: 1.374180\n",
            "Cost after iteration 400: 1.355633\n",
            "Cost after iteration 500: 1.340401\n",
            "Cost after iteration 600: 1.326784\n",
            "Cost after iteration 700: 1.313964\n",
            "Cost after iteration 800: 1.301393\n",
            "Cost after iteration 900: 1.288920\n",
            "Cost after iteration 1000: 1.276489\n",
            "Cost after iteration 1100: 1.263847\n",
            "Cost after iteration 1200: 1.250823\n",
            "Cost after iteration 1300: 1.237526\n",
            "Cost after iteration 1400: 1.223945\n",
            "Cost after iteration 1500: 1.210274\n",
            "Cost after iteration 1600: 1.196388\n",
            "Cost after iteration 1700: 1.182359\n",
            "Cost after iteration 1800: 1.168196\n",
            "Cost after iteration 1900: 1.153965\n",
            "Cost after iteration 2000: 1.139556\n",
            "Cost after iteration 2100: 1.125133\n",
            "Cost after iteration 2200: 1.110575\n",
            "Cost after iteration 2300: 1.095921\n",
            "Cost after iteration 2400: 1.081232\n",
            "Cost after iteration 2500: 1.066513\n",
            "Cost after iteration 2600: 1.051724\n",
            "Cost after iteration 2700: 1.036872\n",
            "Cost after iteration 2800: 1.021799\n",
            "Cost after iteration 2900: 1.006667\n",
            "Cost after iteration 3000: 0.991435\n",
            "Cost after iteration 3100: 0.976299\n",
            "Cost after iteration 3200: 0.961153\n",
            "Cost after iteration 3300: 0.946039\n",
            "Cost after iteration 3400: 0.930802\n",
            "Cost after iteration 3500: 0.915452\n",
            "Cost after iteration 3600: 0.900048\n",
            "Cost after iteration 3700: 0.884717\n",
            "Cost after iteration 3800: 0.869529\n",
            "Cost after iteration 3900: 0.854368\n",
            "Cost after iteration 4000: 0.839405\n",
            "Cost after iteration 4100: 0.824484\n",
            "Cost after iteration 4200: 0.809539\n",
            "Cost after iteration 4300: 0.794698\n",
            "Cost after iteration 4400: 0.779897\n",
            "Cost after iteration 4500: 0.765183\n",
            "Cost after iteration 4600: 0.750568\n",
            "Cost after iteration 4700: 0.736041\n",
            "Cost after iteration 4800: 0.721605\n",
            "Cost after iteration 4900: 0.707186\n",
            "Cost after iteration 5000: 0.693027\n",
            "Cost after iteration 5100: 0.679167\n",
            "Cost after iteration 5200: 0.665460\n",
            "Cost after iteration 5300: 0.651944\n",
            "Cost after iteration 5400: 0.638760\n",
            "Cost after iteration 5500: 0.625891\n",
            "Cost after iteration 5600: 0.613114\n",
            "Cost after iteration 5700: 0.600486\n",
            "Cost after iteration 5800: 0.588202\n",
            "Cost after iteration 5900: 0.576136\n",
            "Cost after iteration 6000: 0.564285\n",
            "Cost after iteration 6100: 0.552790\n",
            "Cost after iteration 6200: 0.541660\n",
            "Cost after iteration 6300: 0.530733\n",
            "Cost after iteration 6400: 0.520042\n",
            "Cost after iteration 6500: 0.509573\n",
            "Cost after iteration 6600: 0.499372\n",
            "Cost after iteration 6700: 0.489484\n",
            "Cost after iteration 6800: 0.479810\n",
            "Cost after iteration 6900: 0.470271\n",
            "Cost after iteration 7000: 0.460996\n",
            "Cost after iteration 7100: 0.451904\n",
            "Cost after iteration 7200: 0.442893\n",
            "Cost after iteration 7300: 0.434090\n",
            "Cost after iteration 7400: 0.425550\n",
            "Cost after iteration 7500: 0.417222\n",
            "Cost after iteration 7600: 0.409061\n",
            "Cost after iteration 7700: 0.401040\n",
            "Cost after iteration 7800: 0.393190\n",
            "Cost after iteration 7900: 0.385516\n",
            "Cost after iteration 8000: 0.378045\n",
            "Cost after iteration 8100: 0.370729\n",
            "Cost after iteration 8200: 0.363662\n",
            "Cost after iteration 8300: 0.356768\n",
            "Cost after iteration 8400: 0.350017\n",
            "Cost after iteration 8500: 0.343453\n",
            "Cost after iteration 8600: 0.337111\n",
            "Cost after iteration 8700: 0.331042\n",
            "Cost after iteration 8800: 0.325156\n",
            "Cost after iteration 8900: 0.319381\n",
            "Cost after iteration 9000: 0.313770\n",
            "Cost after iteration 9100: 0.308331\n",
            "Cost after iteration 9200: 0.303069\n",
            "Cost after iteration 9300: 0.297967\n",
            "Cost after iteration 9400: 0.293004\n",
            "Cost after iteration 9500: 0.288197\n",
            "Cost after iteration 9600: 0.283507\n",
            "Cost after iteration 9700: 0.278894\n",
            "Cost after iteration 9800: 0.274367\n",
            "Cost after iteration 9900: 0.270000\n",
            "Cost after iteration 10000: 0.265707\n",
            "Cost after iteration 10100: 0.261515\n",
            "Cost after iteration 10200: 0.257387\n",
            "Cost after iteration 10300: 0.253321\n",
            "Cost after iteration 10400: 0.249365\n",
            "Cost after iteration 10500: 0.245572\n",
            "Cost after iteration 10600: 0.241887\n",
            "Cost after iteration 10700: 0.238304\n",
            "Cost after iteration 10800: 0.234784\n",
            "Cost after iteration 10900: 0.231322\n",
            "Cost after iteration 11000: 0.227938\n",
            "Cost after iteration 11100: 0.224648\n",
            "Cost after iteration 11200: 0.221424\n",
            "Cost after iteration 11300: 0.218260\n",
            "Cost after iteration 11400: 0.215164\n",
            "Cost after iteration 11500: 0.212100\n",
            "Cost after iteration 11600: 0.209087\n",
            "Cost after iteration 11700: 0.206145\n",
            "Cost after iteration 11800: 0.203284\n",
            "Cost after iteration 11900: 0.200486\n",
            "Cost after iteration 12000: 0.197747\n",
            "Cost after iteration 12100: 0.195087\n",
            "Cost after iteration 12200: 0.192517\n",
            "Cost after iteration 12300: 0.190000\n",
            "Cost after iteration 12400: 0.187542\n",
            "Cost after iteration 12500: 0.185116\n",
            "Cost after iteration 12600: 0.182744\n",
            "Cost after iteration 12700: 0.180425\n",
            "Cost after iteration 12800: 0.178160\n",
            "Cost after iteration 12900: 0.175947\n",
            "Cost after iteration 13000: 0.173754\n",
            "Cost after iteration 13100: 0.171586\n",
            "Cost after iteration 13200: 0.169449\n",
            "Cost after iteration 13300: 0.167310\n",
            "Cost after iteration 13400: 0.165180\n",
            "Cost after iteration 13500: 0.163104\n",
            "Cost after iteration 13600: 0.161042\n",
            "Cost after iteration 13700: 0.159005\n",
            "Cost after iteration 13800: 0.157012\n",
            "Cost after iteration 13900: 0.155015\n",
            "Cost after iteration 14000: 0.153037\n",
            "Cost after iteration 14100: 0.151087\n",
            "Cost after iteration 14200: 0.149177\n",
            "Cost after iteration 14300: 0.147295\n",
            "Cost after iteration 14400: 0.145441\n",
            "Cost after iteration 14500: 0.143615\n",
            "Cost after iteration 14600: 0.141828\n",
            "Cost after iteration 14700: 0.140076\n",
            "Cost after iteration 14800: 0.138353\n",
            "Cost after iteration 14900: 0.136680\n",
            "Cost after iteration 15000: 0.135011\n",
            "Cost after iteration 15100: 0.133352\n",
            "Cost after iteration 15200: 0.131694\n",
            "Cost after iteration 15300: 0.130066\n",
            "Cost after iteration 15400: 0.128458\n",
            "Cost after iteration 15500: 0.126884\n",
            "Cost after iteration 15600: 0.125336\n",
            "Cost after iteration 15700: 0.123791\n",
            "Cost after iteration 15800: 0.122284\n",
            "Cost after iteration 15900: 0.120798\n",
            "Cost after iteration 16000: 0.119340\n",
            "Cost after iteration 16100: 0.117904\n",
            "Cost after iteration 16200: 0.116496\n",
            "Cost after iteration 16300: 0.115095\n",
            "Cost after iteration 16400: 0.113706\n",
            "Cost after iteration 16500: 0.112311\n",
            "Cost after iteration 16600: 0.110928\n",
            "Cost after iteration 16700: 0.109557\n",
            "Cost after iteration 16800: 0.108196\n",
            "Cost after iteration 16900: 0.106867\n",
            "Cost after iteration 17000: 0.105563\n",
            "Cost after iteration 17100: 0.104285\n",
            "Cost after iteration 17200: 0.103026\n",
            "Cost after iteration 17300: 0.101791\n",
            "Cost after iteration 17400: 0.100569\n",
            "Cost after iteration 17500: 0.099358\n",
            "Cost after iteration 17600: 0.098170\n",
            "Cost after iteration 17700: 0.097007\n",
            "Cost after iteration 17800: 0.095859\n",
            "Cost after iteration 17900: 0.094737\n",
            "Cost after iteration 18000: 0.093644\n",
            "Cost after iteration 18100: 0.092568\n",
            "Cost after iteration 18200: 0.091509\n",
            "Cost after iteration 18300: 0.090467\n",
            "Cost after iteration 18400: 0.089442\n",
            "Cost after iteration 18500: 0.088429\n",
            "Cost after iteration 18600: 0.087439\n",
            "Cost after iteration 18700: 0.086463\n",
            "Cost after iteration 18800: 0.085501\n",
            "Cost after iteration 18900: 0.084553\n",
            "Cost after iteration 19000: 0.083607\n",
            "Cost after iteration 19100: 0.082680\n",
            "Cost after iteration 19200: 0.081768\n",
            "Cost after iteration 19300: 0.080848\n",
            "Cost after iteration 19400: 0.079940\n",
            "Cost after iteration 19500: 0.079045\n",
            "Cost after iteration 19600: 0.078150\n",
            "Cost after iteration 19700: 0.077277\n",
            "Cost after iteration 19800: 0.076407\n",
            "Cost after iteration 19900: 0.075541\n",
            "Cost after iteration 20000: 0.074688\n",
            "Cost after iteration 20100: 0.073852\n",
            "Cost after iteration 20200: 0.073029\n",
            "Cost after iteration 20300: 0.072214\n",
            "Cost after iteration 20400: 0.071418\n",
            "Cost after iteration 20500: 0.070628\n",
            "Cost after iteration 20600: 0.069860\n",
            "Cost after iteration 20700: 0.069105\n",
            "Cost after iteration 20800: 0.068365\n",
            "Cost after iteration 20900: 0.067635\n",
            "Cost after iteration 21000: 0.066913\n",
            "Cost after iteration 21100: 0.066201\n",
            "Cost after iteration 21200: 0.065494\n",
            "Cost after iteration 21300: 0.064774\n",
            "Cost after iteration 21400: 0.064076\n",
            "Cost after iteration 21500: 0.063396\n",
            "Cost after iteration 21600: 0.062721\n",
            "Cost after iteration 21700: 0.062055\n",
            "Cost after iteration 21800: 0.061402\n",
            "Cost after iteration 21900: 0.060764\n",
            "Cost after iteration 22000: 0.060140\n",
            "Cost after iteration 22100: 0.059519\n",
            "Cost after iteration 22200: 0.058903\n",
            "Cost after iteration 22300: 0.058299\n",
            "Cost after iteration 22400: 0.057708\n",
            "Cost after iteration 22500: 0.057129\n",
            "Cost after iteration 22600: 0.056560\n",
            "Cost after iteration 22700: 0.056001\n",
            "Cost after iteration 22800: 0.055448\n",
            "Cost after iteration 22900: 0.054902\n",
            "Cost after iteration 23000: 0.054363\n",
            "Cost after iteration 23100: 0.053834\n",
            "Cost after iteration 23200: 0.053307\n",
            "Cost after iteration 23300: 0.052775\n",
            "Cost after iteration 23400: 0.052230\n",
            "Cost after iteration 23500: 0.051698\n",
            "Cost after iteration 23600: 0.051178\n",
            "Cost after iteration 23700: 0.050666\n",
            "Cost after iteration 23800: 0.050163\n",
            "Cost after iteration 23900: 0.049669\n",
            "Cost after iteration 24000: 0.049183\n",
            "Cost after iteration 24100: 0.048707\n",
            "Cost after iteration 24200: 0.048237\n",
            "Cost after iteration 24300: 0.047779\n",
            "Cost after iteration 24400: 0.047333\n",
            "Cost after iteration 24500: 0.046898\n",
            "Cost after iteration 24600: 0.046470\n",
            "Cost after iteration 24700: 0.046049\n",
            "Cost after iteration 24800: 0.045637\n",
            "Cost after iteration 24900: 0.045232\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# GRADED CODE: hyperparameter tuning\n",
        "# suggestion: 4-5 layers, layer node < 1000, learning rate < 0.01, iterations > 2500\n",
        "if bonus:\n",
        "  ### START CODE HERE ### (‚âà 2 lines of code) (bonus)\n",
        "  layers_dims = [X_train.shape[0], 32, 16, 8, 4] #  e.g. [64, 1, 1, 1, 4]\n",
        "  parameters = L_layer_model(X_training, y_training, layers_dims, learning_rate = 0.0075, num_iterations = 25000, print_cost = True, classes=4)\n",
        "  ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI92fh4JXC1k",
        "outputId": "1952e535-de1a-49ac-b4cc-098d84d8b2cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9719546135731106\n"
          ]
        }
      ],
      "source": [
        "if bonus:\n",
        "  pred_train = predict(X_train, y_train, parameters, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehjcfSU2XD3-",
        "outputId": "3ba3947b-4226-44a1-80f6-796464dcf952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8860353130016051\n"
          ]
        }
      ],
      "source": [
        "#You can check for your validation accuracy here. (Optional)\n",
        "if bonus:\n",
        "### START CODE HERE ###\n",
        "  pred_validate = predict(X_validate, y_validate, parameters, 4)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "YHFDuq2BQ2qI"
      },
      "outputs": [],
      "source": [
        "if bonus:\n",
        "  pred_test = predict(X_test, None, parameters, 4)\n",
        "  output[\"bonus_pred_test\"] = pred_test\n",
        "else:\n",
        "  output[\"bonus_pred_test\"] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXGnS3HQeNUc"
      },
      "source": [
        "# Submit prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "twMsmXbQeDL_"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "assert(list(output.keys()) == ['initialize_parameters', 'initialize_parameters_deep', 'linear_forward', 'sigmoid', 'relu', 'softmax', 'linear_activation_forward_sigmoid', 'linear_activation_forward_relu', 'linear_activation_forward_softmax', 'compute_BCE_cost', 'compute_CCE_cost', 'linear_backward', 'sigmoid_backward', 'relu_backward', 'softmax_CCE_backward', 'linear_activation_backward_sigmoid', 'linear_activation_backward_relu', 'update_parameters', 'basic_pred_val', 'bonus_pred_test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "bCJ0XTO_zE8A"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ### (‚âà 2 lines of code)\n",
        "student_id = 106091228\n",
        "np.save(str(student_id) + \"_output.npy\", output)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFBFUUEg1to-",
        "outputId": "e3758d0b-3185-4465-e82f-4d8c89ee8ace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initialize_parametersÔºö <class 'dict'>\n",
            "initialize_parameters_deepÔºö <class 'dict'>\n",
            "linear_forwardÔºö <class 'tuple'>\n",
            "sigmoidÔºö <class 'tuple'>\n",
            "reluÔºö <class 'tuple'>\n",
            "softmaxÔºö <class 'tuple'>\n",
            "linear_activation_forward_sigmoidÔºö <class 'tuple'>\n",
            "linear_activation_forward_reluÔºö <class 'tuple'>\n",
            "linear_activation_forward_softmaxÔºö <class 'tuple'>\n",
            "compute_BCE_costÔºö <class 'numpy.float64'>\n",
            "compute_CCE_costÔºö <class 'numpy.float64'>\n",
            "linear_backwardÔºö <class 'tuple'>\n",
            "sigmoid_backwardÔºö <class 'numpy.ndarray'>\n",
            "relu_backwardÔºö <class 'numpy.ndarray'>\n",
            "softmax_CCE_backwardÔºö <class 'numpy.ndarray'>\n",
            "linear_activation_backward_sigmoidÔºö <class 'tuple'>\n",
            "linear_activation_backward_reluÔºö <class 'tuple'>\n",
            "update_parametersÔºö <class 'dict'>\n",
            "basic_pred_valÔºö <class 'numpy.ndarray'>\n",
            "bonus_pred_testÔºö <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "submit = np.load(str(student_id) + \"_output.npy\", allow_pickle=True).item()\n",
        "for key, value in submit.items():\n",
        "  print(str(key) + \"Ôºö \" + str(type(value)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2zmpUNl9wEJ"
      },
      "source": [
        "Expected output: (without bonus) <br>\n",
        "<small>\n",
        "initialize_parametersÔºö <class 'dict'> <br>\n",
        "initialize_parameters_deepÔºö <class 'dict'> <br>\n",
        "linear_forwardÔºö <class 'tuple'> <br>\n",
        "sigmoidÔºö <class 'tuple'> <br>\n",
        "reluÔºö <class 'tuple'> <br>\n",
        "softmaxÔºö <class 'NoneType'> <br>\n",
        "linear_activation_forward_sigmoidÔºö <class 'tuple'> <br>\n",
        "linear_activation_forward_reluÔºö <class 'tuple'> <br>\n",
        "linear_activation_forward_softmaxÔºö <class 'NoneType'> <br>\n",
        "compute_BCE_costÔºö <class 'numpy.ndarray'> <br>\n",
        "compute_CCE_costÔºö <class 'NoneType'> <br>\n",
        "linear_backwardÔºö <class 'tuple'> <br>\n",
        "sigmoid_backwardÔºö <class 'numpy.ndarray'> <br>\n",
        "relu_backwardÔºö <class 'numpy.ndarray'> <br>\n",
        "softmax_CCE_backwardÔºö <class 'NoneType'> <br>\n",
        "linear_activation_backward_sigmoidÔºö <class 'tuple'> <br>\n",
        "linear_activation_backward_reluÔºö <class 'tuple'> <br>\n",
        "update_parametersÔºö <class 'dict'> <br>\n",
        "basic_pred_valÔºö <class 'numpy.ndarray'> <br>\n",
        "bonus_pred_testÔºö <class 'NoneType'> <br>\n",
        "</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trQqZni7jhP0"
      },
      "source": [
        "Expected output: (with bonus)<br>\n",
        "<small>\n",
        "initialize_parametersÔºö <class 'dict'> <br>\n",
        "initialize_parameters_deepÔºö <class 'dict'> <br>\n",
        "linear_forwardÔºö <class 'tuple'> <br>\n",
        "sigmoidÔºö <class 'tuple'> <br>\n",
        "reluÔºö <class 'tuple'> <br>\n",
        "softmaxÔºö <class 'tuple'> <br>\n",
        "linear_activation_forward_sigmoidÔºö <class 'tuple'> <br>\n",
        "linear_activation_forward_reluÔºö <class 'tuple'> <br>\n",
        "linear_activation_forward_softmaxÔºö <class 'tuple'> <br>\n",
        "compute_BCE_costÔºö <class 'numpy.ndarray'> <br>\n",
        "compute_CCE_costÔºö <class 'numpy.ndarray'> <br>\n",
        "linear_backwardÔºö <class 'tuple'> <br>\n",
        "sigmoid_backwardÔºö <class 'numpy.ndarray'> <br>\n",
        "relu_backwardÔºö <class 'numpy.ndarray'> <br>\n",
        "softmax_CCE_backwardÔºö <class 'numpy.ndarray'> <br>\n",
        "linear_activation_backward_sigmoidÔºö <class 'tuple'> <br>\n",
        "linear_activation_backward_reluÔºö <class 'tuple'> <br>\n",
        "update_parametersÔºö <class 'dict'> <br>\n",
        "basic_pred_valÔºö <class 'numpy.ndarray'> <br>\n",
        "bonus_pred_testÔºö <class 'numpy.ndarray'>\n",
        "</small>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "[106091228]_hw3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
